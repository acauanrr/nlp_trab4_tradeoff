# AnÃ¡lise Quantitativa do Trade-off entre EspecializaÃ§Ã£o e GeneralizaÃ§Ã£o em LLMs via Fine-Tuning

ðŸ”— **[Acesse o notebook no Google Colab](https://colab.research.google.com/drive/1EOtC8O84xTV0062KKlmNGfNHs5b_WbFV?usp=sharing)**

Este repositÃ³rio contÃ©m o cÃ³digo e os resultados do quarto trabalho prÃ¡tico para as disciplinas ICC220 e PPGINF528 da Universidade Federal do Amazonas (UFAM).

**Aluno**: Acauan C. Ribeiro

---

## 1. Objetivo do Projeto

O objetivo central deste projeto foi avaliar empiricamente o processo de fine-tuning em Modelos de Linguagem de Grande Porte (LLMs) para a tarefa de Text-to-SQL. A anÃ¡lise quantifica o ganho de desempenho na tarefa-alvo (usando o dataset Spider) e, simultaneamente, mede a alteraÃ§Ã£o de performance em tarefas de conhecimento geral (usando o dataset MMLU), investigando o trade-off de especializaÃ§Ã£o vs. generalizaÃ§Ã£o.

---

## 2. Estrutura do RepositÃ³rio

```
.
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ lora_config_1.json           # HiperparÃ¢metros para o 1Âº treino
â”‚   â””â”€â”€ lora_config_2.json           # HiperparÃ¢metros para o 2Âº treino
â”œâ”€â”€ custom_metrics/
â”‚   â””â”€â”€ execution_accuracy.py  # MÃ©trica customizada para DeepEval
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ nlp_proj4_new.ipynb          # Notebook Colab utilizado para orquestrar os experimentos
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ spider/
â”‚   â”‚   â”œâ”€â”€ train_formatted.jsonl  # Dataset de treino prÃ©-processado
â”‚   â”‚   â””â”€â”€ ... (outros arquivos do Spider)
â”‚   â””â”€â”€ mmlu_subset/
â”‚       â””â”€â”€ mmlu_150_eval.jsonl    # Subset de avaliaÃ§Ã£o do MMLU
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ lora_config_1/             # Artefatos do treino 1 (checkpoints, etc)
â”‚   â”œâ”€â”€ lora_config_2/             # Artefatos do treino 2 (checkpoints, etc)
â”‚   â””â”€â”€ ... (arquivos .json com as prediÃ§Ãµes)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ preprocess_spider.py     # Script para formatar o dataset Spider
â”‚   â”œâ”€â”€ train_lora.py            # Script para o fine-tuning com LoRA
â”‚   â”œâ”€â”€ eval_spider.py           # Script para avaliaÃ§Ã£o no Spider com a mÃ©trica customizada
â”‚   â””â”€â”€ eval_mmlu.py             # Script para avaliaÃ§Ã£o no MMLU
â”œâ”€â”€ data.zip                     # Arquivo que deve ser descompactado /data
â”œâ”€â”€ requirements.txt             # DependÃªncias do projeto
â””â”€â”€ README.md                    # Este arquivo
```

---

## 3. Como Reproduzir os Resultados

Para reproduzir todos os experimentos, siga os passos abaixo. Recomenda-se o uso de um ambiente com GPU (e.g., Google Colab com GPU T4 ou superior).

### Passo 1: ConfiguraÃ§Ã£o do Ambiente

**Clone o repositÃ³rio:**

```bash
git clone https://github.com/acauanrr/nlp_trab4_tradeoff.git
cd nlp_trab4_tradeoff
```

**Instale as dependÃªncias com as versÃµes exatas para garantir a reprodutibilidade:**

```bash
pip install -r requirements.txt
```

**FaÃ§a o download e descompacte os dados do Spider:**

```bash
# O arquivo data.zip jÃ¡ estÃ¡ no repositÃ³rio
unzip -q data.zip -d .
```

**Autentique-se no Hugging Face para baixar o modelo Llama-3:**

*No seu script ou notebook, execute:*

```python
from huggingface_hub import notebook_login
notebook_login()
# Cole seu token de acesso quando solicitado
```

---

### Passo 2: PrÃ©-processamento dos Dados

Formate o dataset Spider para o padrÃ£o de chat utilizado no treinamento.

```bash
python scripts/preprocess_spider.py
```

Isso irÃ¡ gerar o arquivo `data/spider/train_formatted.jsonl`, que serÃ¡ usado na prÃ³xima etapa.

---

### Passo 3: Treinamento (Fine-Tuning)

Foram testadas duas configuraÃ§Ãµes de hiperparÃ¢metros distintas. Para treinar cada modelo, execute os seguintes comandos:

**Treinamento com a ConfiguraÃ§Ã£o 1 (`lora_config_1.json`):**

```bash
python scripts/train_lora.py     --config_file lora_config_1.json     --output_base_dir results     --maxsteps 2048
```

**Treinamento com a ConfiguraÃ§Ã£o 2 (`lora_config_2.json`):**

```bash
python scripts/train_lora.py     --config_file lora_config_2.json     --output_base_dir results     --maxsteps 2048
```

Ao final, os adaptadores LoRA estarÃ£o salvos em `results/lora_config_1/final_adapter` e `results/lora_config_2/final_adapter`.

---

### Passo 4: AvaliaÃ§Ã£o

#### 4.1 AvaliaÃ§Ã£o na Tarefa-Alvo (Spider)

Para avaliar o modelo base e os modelos fine-tuned no dataset Spider, execute os seguintes comandos. Os resultados da mÃ©trica **Execution Accuracy** serÃ£o impressos no console, e as prediÃ§Ãµes em SQL serÃ£o salvas nos arquivos `.json` especificados.

**AvaliaÃ§Ã£o do Modelo Base:**

```bash
python scripts/eval_spider.py     --mode baseline     --output_file results/spider_baseline_preds.json     --batch_size 8
```

**AvaliaÃ§Ã£o do Modelo Fine-Tuned (Config 1):**

```bash
python scripts/eval_spider.py     --mode finetuned     --lora_adapter_path results/lora_config_1/final_adapter     --output_file results/spider_finetuned_preds_1.json     --batch_size 8
```

**AvaliaÃ§Ã£o do Modelo Fine-Tuned (Config 2):**

```bash
python scripts/eval_spider.py     --mode finetuned     --lora_adapter_path results/lora_config_2/final_adapter     --output_file results/spider_finetuned_preds_2.json     --batch_size 8
```

#### 4.2 AvaliaÃ§Ã£o de GeneralizaÃ§Ã£o (MMLU)

Para medir a regressÃ£o (ou ganho) de capacidade, avalie os trÃªs modelos no nosso subset do MMLU.

**AvaliaÃ§Ã£o do Modelo Base:**

```bash
python scripts/eval_mmlu.py
```

**AvaliaÃ§Ã£o do Modelo Fine-Tuned (Config 1):**

```bash
python scripts/eval_mmlu.py --lora_adapter_path results/lora_config_1/final_adapter
```

**AvaliaÃ§Ã£o do Modelo Fine-Tuned (Config 2):**

```bash
python scripts/eval_mmlu.py --lora_adapter_path results/lora_config_2/final_adapter
```

---

## 4. AnÃ¡lise dos Resultados

A anÃ¡lise completa dos resultados, incluindo as tabelas comparativas e a discussÃ£o sobre o trade-off, estÃ¡ detalhada no relatÃ³rio tÃ©cnico em PDF. Os principais achados sÃ£o:

- **Ganho de EspecializaÃ§Ã£o**:  
  O fine-tuning com LoRA resultou em um ganho massivo de performance na tarefa de Text-to-SQL, com a acurÃ¡cia de execuÃ§Ã£o saltando de **9.17% (modelo base)** para **60.83% (ambos os modelos fine-tuned)**.

- **Trade-off de GeneralizaÃ§Ã£o**:  
  Surpreendentemente, nÃ£o foi observado o fenÃ´meno de *"esquecimento catastrÃ³fico"*. Pelo contrÃ¡rio, ambos os modelos fine-tuned demonstraram uma melhora na performance no teste de conhecimento geral MMLU.  
  O modelo da `config_1` (r=16) teve um ganho de acurÃ¡cia de **+261%**, enquanto o da `config_2` (r=32) teve um ganho de **+185%** em relaÃ§Ã£o ao baseline.

Esses resultados sugerem que, para o **Llama-3** em conjunto com a tÃ©cnica **PEFT LoRA**, a especializaÃ§Ã£o em uma tarefa de raciocÃ­nio complexo como **Text-to-SQL** pode, na verdade, **aprimorar as capacidades lÃ³gicas gerais do modelo**, em vez de degradÃ¡-las.
