/content/nlp_trab4_tradeoff
2025-06-23 01:34:00.647217: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-06-23 01:34:00.665817: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1750642440.687662   31695 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1750642440.694426   31695 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-23 01:34:00.716990: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Carregando modelo base: meta-llama/Meta-Llama-3-8B-Instruct
Loading checkpoint shards: 100% 4/4 [00:17<00:00,  4.46s/it]
Configurando LoRA...
trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196
Carregando dataset de: data/spider/train_formatted.jsonl
Generating train split: 7000 examples [00:00, 138459.82 examples/s]
average_tokens_across_devices is set to True but it is invalid when world size is1. Turn it to False automatically.
Adding EOS to train dataset: 100% 7000/7000 [00:00<00:00, 29057.06 examples/s]
Tokenizing train dataset: 100% 7000/7000 [00:08<00:00, 789.14 examples/s]
Truncating train dataset: 100% 7000/7000 [00:00<00:00, 193462.93 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Iniciando o fine-tuning...
  0% 0/2048 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
{'loss': 0.9231, 'grad_norm': 0.6169754266738892, 'learning_rate': 1.9993223845883496e-05, 'num_tokens': 42375.0, 'mean_token_accuracy': 0.7966538190841674, 'epoch': 0.01}
{'loss': 0.6241, 'grad_norm': 0.5271484851837158, 'learning_rate': 1.9971764367353262e-05, 'num_tokens': 84665.0, 'mean_token_accuracy': 0.8590611565113068, 'epoch': 0.03}
{'loss': 0.5201, 'grad_norm': 0.5843784213066101, 'learning_rate': 1.9935641355205955e-05, 'num_tokens': 128130.0, 'mean_token_accuracy': 0.8726123785972595, 'epoch': 0.04}
{'loss': 0.4786, 'grad_norm': 0.8733852505683899, 'learning_rate': 1.9884907928526965e-05, 'num_tokens': 168464.0, 'mean_token_accuracy': 0.8879089361429214, 'epoch': 0.06}
{'loss': 0.4381, 'grad_norm': 0.9928882718086243, 'learning_rate': 1.9819638691095554e-05, 'num_tokens': 211684.0, 'mean_token_accuracy': 0.8934051942825317, 'epoch': 0.07}
{'loss': 0.3956, 'grad_norm': 1.6179606914520264, 'learning_rate': 1.973992962167956e-05, 'num_tokens': 255730.0, 'mean_token_accuracy': 0.9018684953451157, 'epoch': 0.09}
{'loss': 0.3917, 'grad_norm': 1.2914137840270996, 'learning_rate': 1.9645897932898127e-05, 'num_tokens': 297711.0, 'mean_token_accuracy': 0.9089563077688217, 'epoch': 0.1}
{'loss': 0.316, 'grad_norm': 1.821886658668518, 'learning_rate': 1.9537681898859904e-05, 'num_tokens': 338641.0, 'mean_token_accuracy': 0.9213328021764755, 'epoch': 0.11}
 10% 200/2048 [08:48<1:21:03,  2.63s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
{'loss': 0.2857, 'grad_norm': 1.2477915287017822, 'learning_rate': 1.941544065183021e-05, 'num_tokens': 383241.0, 'mean_token_accuracy': 0.9294198632240296, 'epoch': 0.13}
{'loss': 0.2849, 'grad_norm': 1.9237366914749146, 'learning_rate': 1.927935394822618e-05, 'num_tokens': 422020.0, 'mean_token_accuracy': 0.9272060680389405, 'epoch': 0.14}
{'loss': 0.2622, 'grad_norm': 2.418365955352783, 'learning_rate': 1.9129621904283983e-05, 'num_tokens': 462609.0, 'mean_token_accuracy': 0.9350871515274047, 'epoch': 0.16}
{'loss': 0.2238, 'grad_norm': 2.0323853492736816, 'learning_rate': 1.89664647017868e-05, 'num_tokens': 501203.0, 'mean_token_accuracy': 0.9389592671394348, 'epoch': 0.17}
{'loss': 0.1822, 'grad_norm': 1.5073374509811401, 'learning_rate': 1.8790122264286336e-05, 'num_tokens': 540055.0, 'mean_token_accuracy': 0.9501072281599044, 'epoch': 0.19}
{'loss': 0.1896, 'grad_norm': 0.96651291847229, 'learning_rate': 1.8600853904293904e-05, 'num_tokens': 580686.0, 'mean_token_accuracy': 0.9502330791950225, 'epoch': 0.2}
{'loss': 0.1543, 'grad_norm': 1.5074810981750488, 'learning_rate': 1.8398937941959996e-05, 'num_tokens': 623290.0, 'mean_token_accuracy': 0.9586116290092468, 'epoch': 0.21}
{'loss': 0.141, 'grad_norm': 1.2710247039794922, 'learning_rate': 1.8184671295802987e-05, 'num_tokens': 663455.0, 'mean_token_accuracy': 0.9573544961214066, 'epoch': 0.23}
 20% 400/2048 [17:41<1:14:37,  2.72s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
{'loss': 0.1395, 'grad_norm': 0.6385378241539001, 'learning_rate': 1.7958369046088837e-05, 'num_tokens': 704614.0, 'mean_token_accuracy': 0.9606803381443023, 'epoch': 0.24}
{'loss': 0.1367, 'grad_norm': 2.1445229053497314, 'learning_rate': 1.7720363971503845e-05, 'num_tokens': 749233.0, 'mean_token_accuracy': 0.9618693268299103, 'epoch': 0.26}
{'loss': 0.1287, 'grad_norm': 0.7258260846138, 'learning_rate': 1.7471006059801802e-05, 'num_tokens': 792177.0, 'mean_token_accuracy': 0.963099085688591, 'epoch': 0.27}
{'loss': 0.1102, 'grad_norm': 0.6437020897865295, 'learning_rate': 1.721066199314508e-05, 'num_tokens': 837679.0, 'mean_token_accuracy': 0.9653456646203995, 'epoch': 0.29}
{'loss': 0.1268, 'grad_norm': 1.9754204750061035, 'learning_rate': 1.693971460889654e-05, 'num_tokens': 877918.0, 'mean_token_accuracy': 0.9616095161437989, 'epoch': 0.3}
{'loss': 0.1411, 'grad_norm': 2.829875946044922, 'learning_rate': 1.66585623366551e-05, 'num_tokens': 915901.0, 'mean_token_accuracy': 0.9612751382589341, 'epoch': 0.31}
{'loss': 0.1181, 'grad_norm': 0.6530052423477173, 'learning_rate': 1.6379439036218442e-05, 'num_tokens': 955221.0, 'mean_token_accuracy': 0.9651702636480332, 'epoch': 0.33}
{'loss': 0.1331, 'grad_norm': 1.2895514965057373, 'learning_rate': 1.607949784967774e-05, 'num_tokens': 993869.0, 'mean_token_accuracy': 0.9616555362939835, 'epoch': 0.34}
 29% 600/2048 [26:32<1:03:06,  2.61s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
{'loss': 0.1153, 'grad_norm': 0.4918597638607025, 'learning_rate': 1.5770616728556797e-05, 'num_tokens': 1031412.0, 'mean_token_accuracy': 0.9665810471773147, 'epoch': 0.36}
{'loss': 0.1214, 'grad_norm': 0.4754459857940674, 'learning_rate': 1.5453249884220466e-05, 'num_tokens': 1073429.0, 'mean_token_accuracy': 0.9652917814254761, 'epoch': 0.37}
{'loss': 0.112, 'grad_norm': 2.1043219566345215, 'learning_rate': 1.5127864006335631e-05, 'num_tokens': 1115470.0, 'mean_token_accuracy': 0.9658617085218429, 'epoch': 0.39}
{'loss': 0.1071, 'grad_norm': 0.6845622658729553, 'learning_rate': 1.4794937576601532e-05, 'num_tokens': 1157268.0, 'mean_token_accuracy': 0.9667835944890976, 'epoch': 0.4}
{'loss': 0.1123, 'grad_norm': 0.48240435123443604, 'learning_rate': 1.4454960165139817e-05, 'num_tokens': 1198327.0, 'mean_token_accuracy': 0.9664394545555115, 'epoch': 0.41}
{'loss': 0.1072, 'grad_norm': 0.8977017998695374, 'learning_rate': 1.410843171057904e-05, 'num_tokens': 1238882.0, 'mean_token_accuracy': 0.9680417150259018, 'epoch': 0.43}
{'loss': 0.0987, 'grad_norm': 1.3693022727966309, 'learning_rate': 1.3755861784892176e-05, 'num_tokens': 1279651.0, 'mean_token_accuracy': 0.9697352862358093, 'epoch': 0.44}
{'loss': 0.0966, 'grad_norm': 0.44336915016174316, 'learning_rate': 1.339776884406827e-05, 'num_tokens': 1324574.0, 'mean_token_accuracy': 0.969769030213356, 'epoch': 0.46}
 39% 800/2048 [35:24<55:11,  2.65s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
{'loss': 0.1104, 'grad_norm': 0.4223198890686035, 'learning_rate': 1.3034679465720115e-05, 'num_tokens': 1363957.0, 'mean_token_accuracy': 0.9664253503084183, 'epoch': 0.47}
{'loss': 0.1048, 'grad_norm': 0.8076961040496826, 'learning_rate': 1.2667127574748985e-05, 'num_tokens': 1403169.0, 'mean_token_accuracy': 0.9696536761522293, 'epoch': 0.49}
{'loss': 0.0918, 'grad_norm': 2.991152286529541, 'learning_rate': 1.229565365820519e-05, 'num_tokens': 1445489.0, 'mean_token_accuracy': 0.9703664118051529, 'epoch': 0.5}
{'loss': 0.1062, 'grad_norm': 0.6887510418891907, 'learning_rate': 1.1920803970498924e-05, 'num_tokens': 1487985.0, 'mean_token_accuracy': 0.9677678644657135, 'epoch': 0.51}
{'loss': 0.0978, 'grad_norm': 0.5812679529190063, 'learning_rate': 1.1543129730130202e-05, 'num_tokens': 1532860.0, 'mean_token_accuracy': 0.9699116498231888, 'epoch': 0.53}
{'loss': 0.0949, 'grad_norm': 0.5060136318206787, 'learning_rate': 1.116318630911905e-05, 'num_tokens': 1578303.0, 'mean_token_accuracy': 0.9696228837966919, 'epoch': 0.54}
{'loss': 0.0985, 'grad_norm': 0.7751819491386414, 'learning_rate': 1.0781532416327945e-05, 'num_tokens': 1619107.0, 'mean_token_accuracy': 0.9695989680290222, 'epoch': 0.56}
{'loss': 0.0942, 'grad_norm': 0.2778698205947876, 'learning_rate': 1.03987292758774e-05, 'num_tokens': 1663749.0, 'mean_token_accuracy': 0.9713437396287918, 'epoch': 0.57}
 49% 1000/2048 [44:20<47:06,  2.70s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
{'loss': 0.0976, 'grad_norm': 0.4544927179813385, 'learning_rate': 1.0015339801862849e-05, 'num_tokens': 1703397.0, 'mean_token_accuracy': 0.97235127389431, 'epoch': 0.59}
{'loss': 0.1045, 'grad_norm': 0.6975478529930115, 'learning_rate': 9.631927770586412e-06, 'num_tokens': 1742867.0, 'mean_token_accuracy': 0.9686024433374405, 'epoch': 0.6}
{'loss': 0.1002, 'grad_norm': 0.5770018696784973, 'learning_rate': 9.24905699152079e-06, 'num_tokens': 1781862.0, 'mean_token_accuracy': 0.9689063268899918, 'epoch': 0.61}
{'loss': 0.0905, 'grad_norm': 0.43009042739868164, 'learning_rate': 8.867290478224358e-06, 'num_tokens': 1826839.0, 'mean_token_accuracy': 0.9718733853101731, 'epoch': 0.63}
{'loss': 0.0978, 'grad_norm': 0.42951321601867676, 'learning_rate': 8.4871896204267e-06, 'num_tokens': 1867645.0, 'mean_token_accuracy': 0.9709588468074799, 'epoch': 0.64}
{'loss': 0.1065, 'grad_norm': 1.1455916166305542, 'learning_rate': 8.109313358501939e-06, 'num_tokens': 1906671.0, 'mean_token_accuracy': 0.9685347390174865, 'epoch': 0.66}
{'loss': 0.0928, 'grad_norm': 0.49474433064460754, 'learning_rate': 7.7342173615439e-06, 'num_tokens': 1950111.0, 'mean_token_accuracy': 0.9706578558683395, 'epoch': 0.67}
{'loss': 0.094, 'grad_norm': 1.141781210899353, 'learning_rate': 7.362453210251686e-06, 'num_tokens': 1992894.0, 'mean_token_accuracy': 0.9714528936147689, 'epoch': 0.69}
 59% 1200/2048 [53:13<37:30,  2.65s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
{'loss': 0.0957, 'grad_norm': 0.5376015305519104, 'learning_rate': 6.994567585827268e-06, 'num_tokens': 2036249.0, 'mean_token_accuracy': 0.9704830056428909, 'epoch': 0.7}
{'loss': 0.094, 'grad_norm': 0.5188735723495483, 'learning_rate': 6.631101466077801e-06, 'num_tokens': 2078239.0, 'mean_token_accuracy': 0.9699711811542511, 'epoch': 0.71}
{'loss': 0.0965, 'grad_norm': 1.3485774993896484, 'learning_rate': 6.272589329904843e-06, 'num_tokens': 2118160.0, 'mean_token_accuracy': 0.9707417404651641, 'epoch': 0.73}
{'loss': 0.0888, 'grad_norm': 0.3564460277557373, 'learning_rate': 5.919558371350213e-06, 'num_tokens': 2159508.0, 'mean_token_accuracy': 0.9715082329511643, 'epoch': 0.74}
{'loss': 0.0982, 'grad_norm': 0.2914969027042389, 'learning_rate': 5.572527724354302e-06, 'num_tokens': 2202246.0, 'mean_token_accuracy': 0.9695446628332138, 'epoch': 0.76}
{'loss': 0.0903, 'grad_norm': 0.9627668261528015, 'learning_rate': 5.2320076993667815e-06, 'num_tokens': 2245318.0, 'mean_token_accuracy': 0.9720521694421769, 'epoch': 0.77}
{'loss': 0.0865, 'grad_norm': 0.7154909372329712, 'learning_rate': 4.898499032932335e-06, 'num_tokens': 2288551.0, 'mean_token_accuracy': 0.9720744174718857, 'epoch': 0.79}
{'loss': 0.0877, 'grad_norm': 1.1875861883163452, 'learning_rate': 4.572492151354842e-06, 'num_tokens': 2335825.0, 'mean_token_accuracy': 0.9726879692077637, 'epoch': 0.8}
 68% 1400/2048 [1:02:08<28:40,  2.66s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
{'loss': 0.0946, 'grad_norm': 0.7444067001342773, 'learning_rate': 4.254466449522842e-06, 'num_tokens': 2375452.0, 'mean_token_accuracy': 0.9702365326881409, 'epoch': 0.81}
{'loss': 0.0987, 'grad_norm': 0.347977876663208, 'learning_rate': 3.944889585956746e-06, 'num_tokens': 2414261.0, 'mean_token_accuracy': 0.9699415236711502, 'epoch': 0.83}
{'loss': 0.0987, 'grad_norm': 1.0194916725158691, 'learning_rate': 3.644216795114439e-06, 'num_tokens': 2453424.0, 'mean_token_accuracy': 0.9708739531040191, 'epoch': 0.84}
{'loss': 0.0864, 'grad_norm': 0.5433842539787292, 'learning_rate': 3.352890217966551e-06, 'num_tokens': 2494888.0, 'mean_token_accuracy': 0.9715469568967819, 'epoch': 0.86}
{'loss': 0.0905, 'grad_norm': 0.5754000544548035, 'learning_rate': 3.071338251825753e-06, 'num_tokens': 2535890.0, 'mean_token_accuracy': 0.9722635716199874, 'epoch': 0.87}
{'loss': 0.0809, 'grad_norm': 0.37673836946487427, 'learning_rate': 2.799974920386184e-06, 'num_tokens': 2580853.0, 'mean_token_accuracy': 0.9749203383922577, 'epoch': 0.89}
{'loss': 0.0984, 'grad_norm': 0.49952295422554016, 'learning_rate': 2.5391992648993614e-06, 'num_tokens': 2620038.0, 'mean_token_accuracy': 0.9678143757581711, 'epoch': 0.9}
{'loss': 0.09, 'grad_norm': 0.7139993906021118, 'learning_rate': 2.289394757381864e-06, 'num_tokens': 2661268.0, 'mean_token_accuracy': 0.9717287427186966, 'epoch': 0.91}
 78% 1600/2048 [1:11:00<19:51,  2.66s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
{'loss': 0.0926, 'grad_norm': 0.2564532458782196, 'learning_rate': 2.0509287367176323e-06, 'num_tokens': 2704846.0, 'mean_token_accuracy': 0.9702379083633423, 'epoch': 0.93}
{'loss': 0.0871, 'grad_norm': 0.326386034488678, 'learning_rate': 1.8241518684841642e-06, 'num_tokens': 2746472.0, 'mean_token_accuracy': 0.9707933795452118, 'epoch': 0.94}
{'loss': 0.09, 'grad_norm': 0.29378652572631836, 'learning_rate': 1.6093976292968738e-06, 'num_tokens': 2786865.0, 'mean_token_accuracy': 0.9721743208169937, 'epoch': 0.96}
{'loss': 0.0889, 'grad_norm': 0.6863267421722412, 'learning_rate': 1.4069818164299166e-06, 'num_tokens': 2828507.0, 'mean_token_accuracy': 0.9739438778162003, 'epoch': 0.97}
{'loss': 0.0919, 'grad_norm': 0.4041654169559479, 'learning_rate': 1.2172020834345855e-06, 'num_tokens': 2867302.0, 'mean_token_accuracy': 0.9723506021499634, 'epoch': 0.99}
{'loss': 0.0898, 'grad_norm': 0.4412453770637512, 'learning_rate': 1.040337502438149e-06, 'num_tokens': 2908757.0, 'mean_token_accuracy': 0.9737765926122666, 'epoch': 1.0}
{'loss': 0.0819, 'grad_norm': 0.4380395710468292, 'learning_rate': 8.766481537667726e-07, 'num_tokens': 2951619.0, 'mean_token_accuracy': 0.9735490262508393, 'epoch': 1.01}
{'loss': 0.0965, 'grad_norm': 0.886754035949707, 'learning_rate': 7.263747434959889e-07, 'num_tokens': 2989795.0, 'mean_token_accuracy': 0.9706029623746872, 'epoch': 1.03}
 88% 1800/2048 [1:19:54<10:51,  2.63s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
{'loss': 0.0923, 'grad_norm': 0.6635375022888184, 'learning_rate': 5.897382494911075e-07, 'num_tokens': 3027913.0, 'mean_token_accuracy': 0.971136418581009, 'epoch': 1.04}
{'loss': 0.0828, 'grad_norm': 0.308912456035614, 'learning_rate': 4.6693959645806143e-07, 'num_tokens': 3071877.0, 'mean_token_accuracy': 0.9743047636747361, 'epoch': 1.06}
{'loss': 0.0859, 'grad_norm': 0.7894566655158997, 'learning_rate': 3.5815936048254173e-07, 'num_tokens': 3113282.0, 'mean_token_accuracy': 0.9733356589078903, 'epoch': 1.07}
{'loss': 0.0735, 'grad_norm': 0.5415442585945129, 'learning_rate': 2.6355750349188136e-07, 'num_tokens': 3160868.0, 'mean_token_accuracy': 0.9753248077630997, 'epoch': 1.09}
{'loss': 0.0858, 'grad_norm': 0.6823152303695679, 'learning_rate': 1.8327313803016888e-07, 'num_tokens': 3202495.0, 'mean_token_accuracy': 0.9717644494771958, 'epoch': 1.1}
{'loss': 0.0985, 'grad_norm': 0.750163197517395, 'learning_rate': 1.1742432269250536e-07, 'num_tokens': 3241161.0, 'mean_token_accuracy': 0.9719308537244796, 'epoch': 1.11}
{'loss': 0.1055, 'grad_norm': 0.6767721176147461, 'learning_rate': 6.610788851919348e-08, 'num_tokens': 3276473.0, 'mean_token_accuracy': 0.9678911656141281, 'epoch': 1.13}
{'loss': 0.082, 'grad_norm': 0.30508169531822205, 'learning_rate': 2.939929660517038e-08, 'num_tokens': 3319497.0, 'mean_token_accuracy': 0.9721863466501236, 'epoch': 1.14}
 98% 2000/2048 [1:28:48<02:10,  2.72s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
{'loss': 0.0837, 'grad_norm': 0.524242103099823, 'learning_rate': 7.352527134055765e-09, 'num_tokens': 3359946.0, 'mean_token_accuracy': 0.9733921623229981, 'epoch': 1.16}
{'train_runtime': 5457.8796, 'train_samples_per_second': 1.501, 'train_steps_per_second': 0.375, 'train_loss': 0.15100632008397952, 'num_tokens': 3393486.0, 'mean_token_accuracy': 0.9711983100227688, 'epoch': 1.17}
100% 2048/2048 [1:30:57<00:00,  2.66s/it]
Adaptador LoRA salvo em: results/lora_config_1/final_adapter